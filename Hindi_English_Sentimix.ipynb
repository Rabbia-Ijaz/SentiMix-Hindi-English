{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hindi-English Sentimix.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNSnbU7SBlEe5Txaf0hzVC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rabbia-Ijaz/SentiMix-Hindi-English/blob/main/Hindi_English_Sentimix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTEtn5GUJJdN",
        "outputId": "2c1dea04-29ef-48c2-9476-f5065d1fb336"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlwPuVwXStjT",
        "outputId": "6cebe0e5-d4de-4c26-9a06-ef8f07335a96"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re \n",
        "import nltk\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize  \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "\n",
        "nltk.download('stopwords')\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELkByAhp_qWG"
      },
      "source": [
        "# **Data Cleaning Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VszaRwdYwVp9"
      },
      "source": [
        "#Data Cleaning functions\n",
        "\n",
        "def RemoveNumbers(L): \n",
        "    pattern = '[0-9]'\n",
        "    L = [re.sub(pattern, '', i) for i in L] \n",
        "    return L\n",
        "def RemoveEngStopWords(Sentences):\n",
        "  stop_words = set(stopwords.words('english'))  \n",
        "  for i in range(0,len(Sentences)):\n",
        "    word_tokens = word_tokenize(Sentences[i])  \n",
        "    temp=\"\"\n",
        "    for w in word_tokens:  \n",
        "        if w not in stop_words:  \n",
        "            temp=temp+ w +\" \"\n",
        "    Sentences[i]=temp\n",
        "  return Sentences\n",
        "\n",
        "def RemoveEmoji(Sentences):\n",
        "\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                              u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                              u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                              u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                              u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                              u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                              u\"\\U00002702-\\U000027B0\"\n",
        "                              u\"\\U00002702-\\U000027B0\"\n",
        "                              u\"\\U000024C2-\\U0001F251\"\n",
        "                              u\"\\U0001f926-\\U0001f937\"\n",
        "                              u\"\\U00010000-\\U0010ffff\"\n",
        "                              u\"\\u2640-\\u2642\"\n",
        "                              u\"\\u2600-\\u2B55\"\n",
        "                              u\"\\u200d\"\n",
        "                              u\"\\u23cf\"\n",
        "                              u\"\\u23e9\"\n",
        "                              u\"\\u231a\"\n",
        "                              u\"\\ufe0f\"  # dingbats\n",
        "                              u\"\\u3030\"\n",
        "                              \"]+\", flags=re.UNICODE)\n",
        "  for i in range(0,len(Sentences)):\n",
        "    Sentences[i]=emoji_pattern.sub(r'', Sentences[i])\n",
        "  return Sentences\n",
        "def RemoveChar(Sentences):\n",
        "  for i in range(0,len(Sentences)):\n",
        "    Sentences[i] = re.sub(r'(https? \\/\\/)(\\s)*([\\w\\-]+) (\\s)*(/)*(\\s)*([\\w\\-]+) (/s)*', '', Sentences[i], flags=re.MULTILINE)\n",
        "    Sentences[i]=re.sub('[^A-Za-z0-9 /s]','',Sentences[i])\n",
        "    Sentences[i]=re.sub('[0123456789\\/®³“ë”ïž|£»–¶ï}{%^ã‹à°&*()ðŸ˜><‰™¤—.Šœ‡ªÿ‘\"â€?_¥…!¦@#$,-]', '', Sentences[i])\n",
        "    Sentences[i]=' '.join(Sentences[i].split())\n",
        "  return Sentences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWlSxl3H_Vft"
      },
      "source": [
        "# **File Reading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rkjf8UTA6OZw"
      },
      "source": [
        "#Reading Training Data\n",
        "tweets = []\n",
        "cur = {}\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/train_conll.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for i in f:\n",
        "        i = i.strip().lower()\n",
        "        arr = i.split('\\t')\n",
        "        if len(arr)==3:\n",
        "            tweets.append(cur)\n",
        "            cur = {}\n",
        "            cur['id'] = arr[1]\n",
        "            if arr[2] == 'negative':\n",
        "                cur['bias'] = -1\n",
        "            elif arr[2] == 'positive':\n",
        "                cur['bias'] = 1\n",
        "            elif arr[2] == 'neutral':\n",
        "                cur['bias'] = 0\n",
        "            else:\n",
        "                print('Error here')\n",
        "                print(arr)\n",
        "                break\n",
        "        elif len(arr)==2:\n",
        "            if 'tokens' in cur:\n",
        "                cur['tokens'].append(arr)\n",
        "            else:\n",
        "                cur['tokens'] = [arr]\n",
        "\n",
        "Sentences=[]\n",
        "ID=[]\n",
        "Labels=[]\n",
        "column_names = [\"Id\", \"Sentence\", \"Label\"]\n",
        "TDF = pd.DataFrame(columns = column_names)\n",
        "for i in range (1,len(tweets)):\n",
        "  temp=\"\"\n",
        "  ID.append(tweets[i]['id'])\n",
        "  Labels.append(tweets[i]['bias'])\n",
        "  k=len(tweets[i]['tokens'])\n",
        "  for j in range (0,k):\n",
        "    if (tweets[i]['tokens'])[j][1]!='o' and (tweets[i]['tokens'])[j][1]!='ent':\n",
        "      temp=temp + ((tweets[i]['tokens'])[j][0]+' ')\n",
        "  Sentences.append(temp)\n",
        "  TDF.loc[i-1,'Sentence']=Sentences[i-1]\n",
        "  TDF.loc[i-1,'Id']=ID[i-1]\n",
        "  TDF.loc[i-1,'Label']=Labels[i-1]\n",
        "\n",
        "#Data Cleaning for Train Data\n",
        "Sentences = RemoveEngStopWords(Sentences)\n",
        "Sentences = RemoveEmoji(Sentences)\n",
        "Sentences = RemoveChar(Sentences)\n",
        "Sentences = RemoveNumbers(Sentences)\n",
        "\n",
        "#***************************************************************************#\n",
        "\n",
        "#Reading Test Data\n",
        "Testtweets = []\n",
        "Testcur = {}\n",
        "last=\"@\tBhadasManKi @ mithleshkumarmi @ Kiranja34522516\t@ BJPindia @ sherryontopp Chutiya h madarchod Sabse harami h ye thuk k â€¦ https // t\tco /\t0CcNA0NJqf\"\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/Hindi_test_unalbelled_conll_updated.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for i in f:\n",
        "        i = i.strip().lower()\n",
        "        arr = i.split('\\t')\n",
        "        #print(arr)\n",
        "        if arr[0]=='meta':\n",
        "          Testtweets.append(Testcur)\n",
        "          Testcur = {}\n",
        "          Testcur['id'] = arr[1]\n",
        "        elif arr[0]!='meta':\n",
        "          if 'tokens' in Testcur:\n",
        "              Testcur['tokens'].append(arr)\n",
        "          else:\n",
        "              Testcur['tokens'] = [arr]\n",
        "    \n",
        "\n",
        "TestSentences=[]\n",
        "TestID=[]\n",
        "column_names = [\"Id\", \"Sentence\"]\n",
        "TestTDF = pd.DataFrame(columns = column_names)\n",
        "for i in range (1,len(Testtweets)):\n",
        "  temp=\"\"\n",
        "  TestID.append(Testtweets[i]['id'])\n",
        "  k=len(Testtweets[i]['tokens'])\n",
        "  for j in range (0,k):      \n",
        "    temp=temp + ((Testtweets[i]['tokens'])[j][0]+' ')\n",
        "  TestSentences.append(temp)\n",
        "  TestTDF.loc[i-1,'Sentence']=TestSentences[i-1]\n",
        "  TestTDF.loc[i-1,'Id']=TestID[i-1]\n",
        "TestSentences.append(last)\n",
        "\n",
        "#Data Cleaning for Test Data\n",
        "TestSentences = RemoveEngStopWords(TestSentences)\n",
        "TestSentences = RemoveEmoji(TestSentences)\n",
        "TestSentences = RemoveChar(TestSentences)\n",
        "TestSentences = RemoveNumbers(TestSentences)\n",
        "\n",
        "\n",
        "#***************************************************************************#\n",
        "\n",
        "#Reading Test Labels\n",
        "TestLabels = []\n",
        "TestID=[]\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/test_labels_hinglish.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for i in f:\n",
        "        i = i.strip().lower()\n",
        "        arr = i.split(',')\n",
        "        if arr[0] !='uid':\n",
        "          TestID.append(arr[0])\n",
        "        if arr[1] == 'negative':\n",
        "          TestLabels.append(-1)\n",
        "        elif arr[1] == 'positive':\n",
        "          TestLabels.append(1)\n",
        "        elif arr[1] == 'neutral':\n",
        "          TestLabels.append(0)\n",
        "\n",
        "Y_Train,Y_Test=Labels,TestLabels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD0dHX4CVBC9"
      },
      "source": [
        "# **Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OboEP-hIwRrc",
        "outputId": "53e7cdac-1255-4531-c852-b394214a2599"
      },
      "source": [
        "Max=0\n",
        "F_Pred=0\n",
        "vectorizer = CountVectorizer(max_features=10000, min_df=6, max_df=0.1)\n",
        "X_Train = vectorizer.fit_transform(Sentences).toarray()\n",
        "X_Test=vectorizer.transform(TestSentences).toarray()\n",
        "\n",
        "#Fitting the model\n",
        "SGDC = SGDClassifier()\n",
        "LSVC = LinearSVC()\n",
        "MNB = MultinomialNB()\n",
        "classifier = RandomForestClassifier(n_estimators=600, random_state=1)\n",
        "\n",
        "classifier.fit(X_Train, Y_Train)\n",
        "y_pred=classifier.predict(X_Test)\n",
        "accuracy=metrics.accuracy_score(y_pred, Y_Test)\n",
        "if Max<accuracy:\n",
        "  Max=accuracy\n",
        "  F_Pred=y_pred\n",
        "print ('accuracy_score_randforest = '+str('{:04.2f}'.format(accuracy*100))+'%')\n",
        "\n",
        "LSVC.fit(X_Train, Y_Train)\n",
        "y_pred=LSVC.predict(X_Test)\n",
        "accuracy_score_lsvc_CV = metrics.accuracy_score(y_pred, Y_Test)\n",
        "if Max<accuracy_score_lsvc_CV:\n",
        "  Max=accuracy_score_lsvc_CV\n",
        "  F_Pred=y_pred\n",
        "print('accuracy_score_lsvc_cv = '+str('{:4.2f}'.format(accuracy_score_lsvc_CV*100))+'%')\n",
        "\n",
        "SGDC.fit(X_Train, Y_Train)\n",
        "y_pred=SGDC.predict(X_Test)\n",
        "accuracy_score_sgdc_CV = metrics.accuracy_score(y_pred, Y_Test)\n",
        "if Max<accuracy_score_sgdc_CV:\n",
        "  Max=accuracy_score_sgdc_CV\n",
        "  F_Pred=y_pred\n",
        "print('accuracy_score_sgdc_cv = '+str('{:4.2f}'.format(accuracy_score_sgdc_CV*100))+'%')\n",
        "\n",
        "MNB.fit(X_Train, Y_Train)\n",
        "y_pred=MNB.predict(X_Test)\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, Y_Test)\n",
        "if Max<accuracy_score_mnb:\n",
        "  Max=accuracy_score_mnb\n",
        "  F_Pred=y_pred\n",
        "print('accuracy_score_mnb_cv = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score_randforest = 66.27%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy_score_lsvc_cv = 59.53%\n",
            "accuracy_score_sgdc_cv = 61.90%\n",
            "accuracy_score_mnb_cv = 65.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO6UNVMJKpaH",
        "outputId": "fc3106a7-25f6-47d7-a55e-aac11ad564c5"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X_Train = vectorizer.fit_transform(Sentences).toarray()\n",
        "X_Test=vectorizer.transform(TestSentences).toarray()\n",
        "\n",
        "#on TF-IDF data\n",
        "LSVC.fit(X_Train, Y_Train)\n",
        "y_pred=LSVC.predict(X_Test)\n",
        "accuracy_score_lsvc = metrics.accuracy_score(y_pred, Y_Test)\n",
        "if Max<accuracy_score_lsvc:\n",
        "  Max=accuracy_score_lsvc\n",
        "  F_Pred=y_pred\n",
        "print('accuracy_score_lsvc = '+str('{:4.2f}'.format(accuracy_score_lsvc*100))+'%')\n",
        "\n",
        "SGDC.fit(X_Train, Y_Train)\n",
        "y_pred=SGDC.predict(X_Test)\n",
        "accuracy_score_sgdc = metrics.accuracy_score(y_pred, Y_Test)\n",
        "if Max<accuracy_score_sgdc:\n",
        "  Max=accuracy_score_sgdc\n",
        "print('accuracy_score_sgdc = '+str('{:4.2f}'.format(accuracy_score_sgdc*100))+'%')\n",
        "\n",
        "MNB.fit(X_Train, Y_Train)\n",
        "y_pred=MNB.predict(X_Test)\n",
        "accuracy_score_mnb = metrics.accuracy_score(y_pred, Y_Test)\n",
        "if Max<accuracy_score_mnb:\n",
        "  Max=accuracy_score_mnb\n",
        "  F_Pred=y_pred\n",
        "print('accuracy_score_mnb = '+str('{:4.2f}'.format(accuracy_score_mnb*100))+'%')\n",
        "\n",
        "print (Max)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score_lsvc = 63.83%\n",
            "accuracy_score_sgdc = 65.53%\n",
            "accuracy_score_mnb = 65.50%\n",
            "0.6626666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8vfr9w7dz6T"
      },
      "source": [
        "File='/content/drive/MyDrive/Colab Notebooks/answer.txt'\n",
        "f = open(File, \"w\")\n",
        "f.write(\"Uid,Sentiment\\n\")\n",
        "#\"This is line %d\\r\" % (i+1)\n",
        "i=0\n",
        "for i in range(0,len(TestID)-1):\n",
        "  if F_Pred[i]== 1:\n",
        "    f.write(str(TestID[i])+\",positive\\n\")\n",
        "  elif F_Pred[i]==-1:\n",
        "    f.write(str(TestID[i])+\",negative\\n\")\n",
        "  elif F_Pred[i]==0:\n",
        "    f.write(str(TestID[i])+\",neutral\\n\")\n",
        "i=len(TestID)-1\n",
        "if F_Pred[i]== 1:\n",
        "  f.write(str(TestID[i])+\",positive\")\n",
        "elif F_Pred[i]==-1:\n",
        "  f.write(str(TestID[i])+\",negative\")\n",
        "elif F_Pred[i]==0:\n",
        "  f.write(str(TestID[i])+\",neutral\")\n",
        "f.close()\n",
        "\n",
        "#open and read the file after the appending:\n",
        "#f = open(\"/content/drive/MyDrive/Colab Notebooks/answer.txt\", \"r\")\n",
        "#print(f.read())\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}